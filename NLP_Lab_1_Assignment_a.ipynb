{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Lab 1 Assignment"
      ],
      "metadata": {
        "id": "LFF_EkTpOT8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary libraries"
      ],
      "metadata": {
        "id": "ejGgKtoKObdH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mMsDENmDOS8a"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords,wordnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7p61Cqq1OteQ",
        "outputId": "f21e1af4-6300-4098-8f89-fa9edb5939d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#init sample text\n",
        "text=\"The leaves on the tree are falling and the wind is blowing gently. This is a beautiful sight to behold\""
      ],
      "metadata": {
        "id": "iM4J9freO1aQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization - Sentence Tokenization"
      ],
      "metadata": {
        "id": "Ii0ilz9rPNXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "sentences=sent_tokenize(text) #raised error- had to download punkt-tab resource\n",
        "print(\"Sentence tokenization:\")\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enyFlM0fPLUv",
        "outputId": "80936319-52a3-41a1-b3c4-b3ce27c00660"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence tokenization:\n",
            "['The leaves on the tree are falling and the wind is blowing gently.', 'This is a beautiful sight to behold']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization - Word tokenization"
      ],
      "metadata": {
        "id": "f2EMBs7tPy2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=word_tokenize(text)\n",
        "print('\\nWord Tokenization')\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLRyWuZYPX4Y",
        "outputId": "367f1fa2-87dc-4951-dc19-f480549d815d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Tokenization\n",
            "['The', 'leaves', 'on', 'the', 'tree', 'are', 'falling', 'and', 'the', 'wind', 'is', 'blowing', 'gently', '.', 'This', 'is', 'a', 'beautiful', 'sight', 'to', 'behold']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing stopwords"
      ],
      "metadata": {
        "id": "5MqVG4hvP_z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=set(stopwords.words('english'))\n",
        "filtered_words=[word for word in words if word.lower() not in stop_words]\n",
        "print(\"\\nWords after removing stop words:\")\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4gL8Z2rP9HP",
        "outputId": "480fc72d-9dad-4e51-b09e-ebf69b6bf8d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Words after removing stop words:\n",
            "['leaves', 'tree', 'falling', 'wind', 'blowing', 'gently', '.', 'beautiful', 'sight', 'behold']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using wordnet"
      ],
      "metadata": {
        "id": "QvX2b0z3QRev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ],
      "metadata": {
        "id": "75ZofjN7QVif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "lemmatized_words=[lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "print(\"\\nLemmatized Words\")\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl1Eq90BQQg4",
        "outputId": "a50e8bfc-8c20-4c47-bf35-dccd160ee883"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatized Words\n",
            "['leaf', 'tree', 'falling', 'wind', 'blowing', 'gently', '.', 'beautiful', 'sight', 'behold']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Synonyms and antonyms using wordnet"
      ],
      "metadata": {
        "id": "nayr-xLqQpd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word=\"beautiful\"\n",
        "synonyms=[]\n",
        "antonyms=[]\n",
        "\n",
        "for syn in wordnet.synsets(word):\n",
        "  for lemma in syn.lemmas():\n",
        "    synonyms.append(lemma.name())\n",
        "    if lemma.antonyms():\n",
        "      antonyms.append(lemma.antonyms()[0].name())"
      ],
      "metadata": {
        "id": "pgWW-AhMQoF4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nSynonyms of '{word}': {set(synonyms)}\")\n",
        "print(f\"Antonyms of '{word}': {set(antonyms)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llZUlw37RADg",
        "outputId": "23a303a7-6381-4fe5-8798-ed3e9ea25962"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Synonyms of 'beautiful': {'beautiful'}\n",
            "Antonyms of 'beautiful': {'ugly'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definitions from Wordnet"
      ],
      "metadata": {
        "id": "b4sOwe8qRTJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "definitions=[syn.definition() for syn in wordnet.synsets(word)]\n",
        "print(f\"\\nDefn of '{word}':\")\n",
        "for i,definition in enumerate(definitions,1):\n",
        "  print(f\"{i}, {definition}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ7nY869RIRP",
        "outputId": "19ade634-92b0-4b99-9e24-67a2a8680dae"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Defn of 'beautiful':\n",
            "1, delighting the senses or exciting intellectual or emotional admiration\n",
            "2, (of weather) highly enjoyable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming using nltk"
      ],
      "metadata": {
        "id": "EGqLtOnRSiPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer,LancasterStemmer\n",
        "\n",
        "porter_stemmer=PorterStemmer()\n",
        "lancaster_stemmer=LancasterStemmer()\n",
        "\n",
        "words=[\"running\",\"ran\",\"runs\",\"easily\",\"fairness\"]\n",
        "\n",
        "porter_stems=[porter_stemmer.stem(word) for word in words]\n",
        "print(\"Stemming with porterstemmer\")\n",
        "print(porter_stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCIAhBL7RqSo",
        "outputId": "61d898d0-9463-4444-cc9a-d458b4833a5e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming with porterstemmer\n",
            "['run', 'ran', 'run', 'easili', 'fair']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lancaster_stems=[lancaster_stemmer.stem(word) for word in words]\n",
        "print('\\nStemming with LancasterStemmer:')\n",
        "print(lancaster_stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxaNixSmTFG4",
        "outputId": "aed3cb08-0e9a-428e-8f49-d5b7d4e5411e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemming with LancasterStemmer:\n",
            "['run', 'ran', 'run', 'easy', 'fair']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word replacement"
      ],
      "metadata": {
        "id": "Qfu6Zw3bTXcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"U r going 2 the mall aren't u?\""
      ],
      "metadata": {
        "id": "mQtQNYSZTTB_"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacements={\n",
        "    \"U \":\" you \",\n",
        "    \" r \":\" are \",\n",
        "    \" 2 \":\" to \",\n",
        "    ' u': \" you\",\n",
        "}"
      ],
      "metadata": {
        "id": "AVcrnBs_TeeO"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key,value in replacements.items():\n",
        "  text=text.replace(key,value)"
      ],
      "metadata": {
        "id": "TODmrmKRTmqP"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"After replacement :\",text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5DtQ8PKTrou",
        "outputId": "76f01fef-bc8e-40f7-f87d-506714b5bef3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After replacement :  you are going to the mall aren't you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that r in any part of the sentence is being replaced by are and that's not actually right"
      ],
      "metadata": {
        "id": "ioIx7JB5UhQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Synonym replacement"
      ],
      "metadata": {
        "id": "_58B8cy5Uo1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "A46dzVonTvx-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synonyms(word):\n",
        "  synonyms=[]\n",
        "  for synset in wordnet.synsets(word):\n",
        "    for lemma in synset.lemmas():\n",
        "      if lemma.name()!=word:\n",
        "        synonyms.append(lemma.name())\n",
        "  return set(synonyms)"
      ],
      "metadata": {
        "id": "s2PjPhFWUt9O"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word=\"happy\"\n",
        "synonyms=get_synonyms(word)\n",
        "print(f\"Synonys of '{word}':\",synonyms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5rUo-nGVCFO",
        "outputId": "447b5c76-2e48-4992-e535-6f5ecfc5d7a1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonys of 'happy': {'felicitous', 'well-chosen', 'glad'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Antonym Replacement"
      ],
      "metadata": {
        "id": "vpKsNXHXVe12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_antonyms(word):\n",
        "  antonyms=[]\n",
        "  for synset in wordnet.synsets(word):\n",
        "    for lemma in synset.lemmas():\n",
        "      if lemma.antonyms():\n",
        "        antonyms.append(lemma.antonyms()[0].name())\n",
        "  return set(antonyms)\n"
      ],
      "metadata": {
        "id": "l9NInJtZVNFm"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word=\"happy\"\n",
        "antonyms=get_antonyms(word)\n",
        "print(f\"Antonyms of '{word}:'\",antonyms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yuPtB0UWoVm",
        "outputId": "ee8cfc28-56f8-4066-eb0a-49d4540def7b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antonyms of 'happy:' {'unhappy'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parse Tree"
      ],
      "metadata": {
        "id": "1J8RC-cLWzx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import CFG\n",
        "\n",
        "grammar = CFG.fromstring('''\n",
        "          S -> NP VP\n",
        "          NP -> DT NN | DT JJ NN\n",
        "          VP -> VBZ NP | VBZ\n",
        "          DT -> 'the' | 'a'\n",
        "          JJ -> 'red'\n",
        "          NN -> 'cat'|'dog'\n",
        "          VBZ -> 'chases'|'sleeps'\n",
        "''')"
      ],
      "metadata": {
        "id": "2QhDj91UWxbW"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser=nltk.ChartParser(grammar)\n",
        "sentence='the red cat chases the dog'.split()\n",
        "\n",
        "print(\"Parse Tree\")\n",
        "for tree in parser.parse(sentence):\n",
        "  print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSeqqhQdXVe-",
        "outputId": "4ca3df54-400c-4edf-9ddd-611bb0699daf"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse Tree\n",
            "(S\n",
            "  (NP (DT the) (JJ red) (NN cat))\n",
            "  (VP (VBZ chases) (NP (DT the) (NN dog))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6uCCBlzXq7m",
        "outputId": "49257cfd-6df5-435f-aa0e-9b0bb43a9882"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         S                    \n",
            "      ___|__________           \n",
            "     |              VP        \n",
            "     |         _____|___       \n",
            "     NP       |         NP    \n",
            "  ___|___     |      ___|___   \n",
            " DT  JJ  NN  VBZ    DT      NN\n",
            " |   |   |    |     |       |  \n",
            "the red cat chases the     dog\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition"
      ],
      "metadata": {
        "id": "rz43cwsdX6s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "text='Barack Obama was the 44th president of the united states and he lives in washington'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJKAKOBAX0mt",
        "outputId": "24e56246-38d3-4813-c9f0-83b46fbdbb35"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "tokens=word_tokenize(text)\n",
        "pos_tags=pos_tag(tokens)\n",
        "named_entities=(ne_chunk(pos_tags))\n",
        "print(\"Named Entity Recog\")\n",
        "for subtree in named_entities:\n",
        "  if isinstance(subtree,Tree):\n",
        "    entity_name=\"\".join([token for token,pos in subtree.leaves()])\n",
        "    entity_type=subtree.label()\n",
        "    print(f\"Entity:{entity_name},Type:{entity_type}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo4mfx9HYYhG",
        "outputId": "42e12d93-a035-448e-8ec4-11d3c60159a9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entity Recog\n",
            "Entity:Barack,Type:PERSON\n",
            "Entity:Obama,Type:PERSON\n"
          ]
        }
      ]
    }
  ]
}